{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ffc6a2b",
      "metadata": {
        "id": "2ffc6a2b"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/low_level/oss_ingestion_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff7db9e-fbf9-4394-9958-35323799a4e3",
      "metadata": {
        "id": "dff7db9e-fbf9-4394-9958-35323799a4e3"
      },
      "source": [
        "# Building RAG from Scratch (Open-source only!)\n",
        "\n",
        "In this tutorial, we show you how to build a data ingestion pipeline into a vector database, and then build a retrieval pipeline from that vector database, from scratch.\n",
        "\n",
        "Notably, we use a fully open-source stack:\n",
        "\n",
        "- Sentence Transformers as the embedding model\n",
        "- Elasticsearch as the vector store (we support many other [vector stores](https://gpt-index.readthedocs.io/en/stable/module_guides/storing/vector_stores.html) too!)\n",
        "- Llama 2 as the LLM (through [llama.cpp](https://github.com/ggerganov/llama.cpp))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25764729-40ba-400f-b0f8-08fb9e8bb74a",
      "metadata": {
        "id": "25764729-40ba-400f-b0f8-08fb9e8bb74a"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We setup our open-source components.\n",
        "1. Sentence Transformers\n",
        "2. Llama 2\n",
        "3. We initialize postgres and wrap it with our wrappers/abstractions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63935557-a11c-4a22-9248-9c746cc89c4c",
      "metadata": {
        "id": "63935557-a11c-4a22-9248-9c746cc89c4c"
      },
      "source": [
        "#### Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7108d3f",
      "metadata": {
        "id": "b7108d3f"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-llms-llama-cpp\n",
        "%pip install llama-index-vector-stores-elasticsearch\n",
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919ae1a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4c08162e-5a48-424c-921f-c9e84a59c72f",
      "metadata": {
        "id": "4c08162e-5a48-424c-921f-c9e84a59c72f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# sentence transformers\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df10089c-917e-4191-a718-0ef7149a6a1e",
      "metadata": {
        "id": "df10089c-917e-4191-a718-0ef7149a6a1e"
      },
      "source": [
        "#### Llama CPP\n",
        "\n",
        "In this notebook, we use the [`llama-2-chat-7b-gguf`](https://huggingface.co/TheBloke/Llama-2-7B-chat-GGUF) model, along with the proper prompt formatting.\n",
        "\n",
        "Check out our [Llama CPP guide](https://gpt-index.readthedocs.io/en/stable/examples/llm/llama_2_llama_cpp.html) for full setup instructions/details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f8a556-9f37-42a3-a88a-f688ad355ee5",
      "metadata": {
        "id": "85f8a556-9f37-42a3-a88a-f688ad355ee5",
        "outputId": "bb31c02f-b422-4222-8f46-d585652da273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /Users/jerryliu/Programming/gpt_index/.venv/lib/python3.10/site-packages (0.2.7)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /Users/jerryliu/Programming/gpt_index/.venv/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/jerryliu/Programming/gpt_index/.venv/lib/python3.10/site-packages (from llama-cpp-python) (4.7.1)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /Users/jerryliu/Programming/gpt_index/.venv/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3cb975f6-c192-4a26-ae50-e9a319d2a66b",
      "metadata": {
        "id": "3cb975f6-c192-4a26-ae50-e9a319d2a66b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q3_K_S.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 11\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q3_K:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q3_K - Small\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.75 GiB (3.50 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  2811.02 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4000\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  2000.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2000.00 MiB, K (f16): 1000.00 MiB, V (f16): 1000.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   289.82 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '11', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "\n",
        "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
        "model_path = \"llama-2-7b-chat.Q3_K_S.gguf\"\n",
        "\n",
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    # model_url=model_url,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=4000,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 0},\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba02cfe2-8b51-4e01-a840-d6508c76ade3",
      "metadata": {
        "id": "ba02cfe2-8b51-4e01-a840-d6508c76ade3"
      },
      "source": [
        "#### Initialize Elasticsearch\n",
        "\n",
        "Using an existing elasticsearch running at localhost, create the database we'll be using.\n",
        "\n",
        "**NOTE**: Of course there are plenty of other open-source/self-hosted databases you can use! e.g. Chroma, Qdrant, Weaviate, and many more. Take a look at our [vector store guide](https://gpt-index.readthedocs.io/en/stable/module_guides/storing/vector_stores.html).\n",
        "\n",
        "Use this command in the terminal to launch a docker hosted Elasticsearch container\n",
        "\n",
        "```\n",
        "docker run -p 9200:9200 \\\n",
        "  -e \"discovery.type=single-node\" \\\n",
        "  -e \"xpack.security.enabled=false\" \\\n",
        "  -e \"xpack.security.http.ssl.enabled=false\" \\\n",
        "  -e \"xpack.license.self_generated.type=trial\" \\\n",
        "  docker.elastic.co/elasticsearch/elasticsearch:8.9.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "53e49964",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: elasticsearch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (8.13.0)\n",
            "Requirement already satisfied: elastic-transport<9,>=8.13 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from elasticsearch) (8.13.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.2.1)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install elasticsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "758b2315",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
        "\n",
        "vector_store = ElasticsearchStore(\n",
        "    index_name=\"qna_mental_health\",\n",
        "    es_url=\"http://localhost:9200\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65e0a69-0668-4df4-a809-71f38695cfea",
      "metadata": {
        "id": "e65e0a69-0668-4df4-a809-71f38695cfea"
      },
      "source": [
        "## Build an Ingestion Pipeline from Scratch\n",
        "\n",
        "We show how to build an ingestion pipeline as mentioned in the introduction.\n",
        "\n",
        "We fast-track the steps here (can skip metadata extraction). More details can be found [in our dedicated ingestion guide](https://gpt-index.readthedocs.io/en/latest/examples/low_level/ingestion.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48febfa0-6a5a-44c9-900e-4316c35d8e81",
      "metadata": {
        "id": "48febfa0-6a5a-44c9-900e-4316c35d8e81"
      },
      "source": [
        "### 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d9c64ce4-b778-4f3b-bc7e-266e0e124308",
      "metadata": {
        "id": "d9c64ce4-b778-4f3b-bc7e-266e0e124308"
      },
      "outputs": [],
      "source": [
        "file_path_test=\"counsel-chat-best-answer-test.csv\"\n",
        "file_path_train=\"counsel-chat-best-answer-train.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_test = pd.read_csv(file_path_test)\n",
        "df_train = pd.read_csv(file_path_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c001b7c-3e79-4d11-bd0e-dc774da25de1",
      "metadata": {
        "id": "9c001b7c-3e79-4d11-bd0e-dc774da25de1"
      },
      "source": [
        "### 2. Use a Text Splitter to Split Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "acd46830",
      "metadata": {},
      "outputs": [],
      "source": [
        "def combined_data(data):\n",
        "    return {\n",
        "        \"question\": data['questionText'],\n",
        "        \"answer\": data['answerText'],\n",
        "        \"topic\": data['topic']\n",
        "    }\n",
        "\n",
        "df_train['qa_mental_health'] = df_train.apply(combined_data, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "855db0bf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': \"I have been diagnosed with general anxiety and depression by my family doctor. They wrote a prescription for me to have an emotional support dog, I have the paper work, and I gave it to my apartment manager. They said I can't keep the ESD because I'm not disabled. What do you suggest I do?\",\n",
              " 'answer': 'This can be a difficult situation. \\xa0Typically, only animals that are specifically trains to accomplish a specific task are legally protected as Service Animsls. Even though that can be very helpful, emotional support animals are not generally protected in the same way.You might not be able to make your landlord accommodate you. If possible, you may want to consider a different apparent that is more animal friendly.',\n",
              " 'topic': 'depression'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['qa_mental_health'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "765d3b20",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = pd.DataFrame(df_train['qa_mental_health'])\n",
        "df_final['question_title'] = df_train['questionTitle']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d25c6539",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qa_mental_health</th>\n",
              "      <th>question_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'question': 'I have been diagnosed with gener...</td>\n",
              "      <td>My apartment manager won't let me keep an emot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'question': 'There are many people willing to...</td>\n",
              "      <td>Why do I feel like I don't belong anywhere?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'question': 'My girlfriend just quit drinking...</td>\n",
              "      <td>How can I help my girlfriend?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'question': 'I don't know how else to explain...</td>\n",
              "      <td>How do I stop feeling empty?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'question': 'I tried telling my husband I was...</td>\n",
              "      <td>How can I get my husband to listen to my needs...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    qa_mental_health  \\\n",
              "0  {'question': 'I have been diagnosed with gener...   \n",
              "1  {'question': 'There are many people willing to...   \n",
              "2  {'question': 'My girlfriend just quit drinking...   \n",
              "3  {'question': 'I don't know how else to explain...   \n",
              "4  {'question': 'I tried telling my husband I was...   \n",
              "\n",
              "                                      question_title  \n",
              "0  My apartment manager won't let me keep an emot...  \n",
              "1        Why do I feel like I don't belong anywhere?  \n",
              "2                      How can I help my girlfriend?  \n",
              "3                       How do I stop feeling empty?  \n",
              "4  How can I get my husband to listen to my needs...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aaae403-d46e-450b-b19f-27ca66e28f1c",
      "metadata": {
        "id": "5aaae403-d46e-450b-b19f-27ca66e28f1c"
      },
      "source": [
        "### 3. Manually Construct Nodes from Text Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ee261129-0f56-4672-9804-a38ea05244cb",
      "metadata": {
        "id": "ee261129-0f56-4672-9804-a38ea05244cb"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import TextNode\n",
        "\n",
        "nodes = []\n",
        "for ans, title in zip(df_final['qa_mental_health'], df_final['question_title']):\n",
        "    node = TextNode(\n",
        "        text=str(ans),\n",
        "    )\n",
        "    node.metadata = {\"title\": title}\n",
        "    nodes.append(node)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65eac30b-27ec-4206-947c-81104dc8babe",
      "metadata": {
        "id": "65eac30b-27ec-4206-947c-81104dc8babe"
      },
      "source": [
        "### 4. Generate Embeddings for each Node\n",
        "\n",
        "Here we generate embeddings for each Node using a sentence_transformers model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "82ef7573-a608-420d-91ab-9fbf17af3e9d",
      "metadata": {
        "id": "82ef7573-a608-420d-91ab-9fbf17af3e9d"
      },
      "outputs": [],
      "source": [
        "for node in nodes:\n",
        "    node_embedding = embed_model.get_text_embedding(\n",
        "        node.get_content(metadata_mode=\"all\")\n",
        "    )\n",
        "    node.embedding = node_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fe354a-4f87-4b13-aae4-f7b7b1fe118e",
      "metadata": {
        "id": "e7fe354a-4f87-4b13-aae4-f7b7b1fe118e"
      },
      "source": [
        "### 5. Load Nodes into a Vector Store\n",
        "\n",
        "We now insert these nodes into our `ElasticsearchVectorStore`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "44add008-616b-4e47-8f61-553befeb7ca4",
      "metadata": {
        "id": "44add008-616b-4e47-8f61-553befeb7ca4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['8bb12a5e-ff00-4548-9ad3-6452376f7490',\n",
              " '19c84223-b0af-499f-91ad-d02e6b668572',\n",
              " '9392cbb0-52e1-4cec-8f24-c1c22133dbfb',\n",
              " '9f114635-e505-479f-bdd8-9ce5bdc961d9',\n",
              " '3538156f-3fe0-4b7a-9128-229b341d0e8d',\n",
              " '055e1e8a-8da3-4bd7-8f14-059bfb456545',\n",
              " '42328b14-7b85-4f72-8ae3-6b5c2cb6f2de',\n",
              " 'd8950f34-c10a-45e8-89e2-9af6bff772ef',\n",
              " 'ce653213-2c85-4883-a6b9-8cef3e24243e',\n",
              " 'a9c9d511-8f0f-4eb0-ac15-5e6c1a95e257',\n",
              " 'ff5e121a-d325-45fe-9439-d4e93236b14f',\n",
              " '108c81de-b6c0-48db-a449-0e9ba63c2103',\n",
              " '88c004cf-1ea0-4f39-940b-766a31130045',\n",
              " '22efc545-66ec-4311-b1a2-f262b41132df',\n",
              " '767e8553-1cf7-4286-9c73-a8060b783e51',\n",
              " '0607ebfa-3c6b-4d29-bf33-c8a57d8bbb4a',\n",
              " 'd830b38d-4a67-464b-a83d-f66461ae61e0',\n",
              " 'e788d21c-e694-41ff-a1fd-08b298f69811',\n",
              " '316fad1f-930d-434e-9af2-3d8528c67f32',\n",
              " '9fc6dbd9-4e38-44c0-a804-e38b832aa8cd',\n",
              " '0dcc145b-f50e-471b-8f76-42a968369df2',\n",
              " '039a652f-a2b7-4f18-a2f0-511f6013916e',\n",
              " 'da315144-5ae7-4eb5-8388-9c394fcd7cac',\n",
              " 'fd4f2465-b0df-461c-89f7-94ac0cd3eeaf',\n",
              " 'fe8363b6-12dd-44fc-8264-dca8f33b0c0a',\n",
              " '70e07a2c-04ad-4041-b1d4-964666af59b0',\n",
              " 'fa0ddcdc-4517-466c-b720-3b75bbcc27ce',\n",
              " '60c64291-2309-419b-8585-95985adb04fb',\n",
              " '8035ae4e-097c-4bb5-aebc-df1bcd66e2c2',\n",
              " 'd32e22f6-59f2-47c7-83b7-794b5bfa4d36',\n",
              " '1661cf26-62a2-48e9-8e9c-45915b894bee',\n",
              " '09d8d0bc-d01c-415f-ba19-a3604968e939',\n",
              " '59832426-1bf7-478b-b111-bb70ecd628c8',\n",
              " '12539fac-c330-4324-8628-31b4091d29ce',\n",
              " '9b0cf0fe-93c7-4764-a1bb-ecbc3654202b',\n",
              " '4ea81d38-8bdd-4738-8acc-5497d52a8e6b',\n",
              " '1612953b-2e87-4580-b97f-c23e93313a18',\n",
              " '5681d420-16cb-4a2f-9d08-8fe1fe66ac57',\n",
              " '92678ae8-a0d7-4ac1-bba2-fdac74c863af',\n",
              " '0e3ccbbc-f950-429c-8f0f-5ab5c083b9e3',\n",
              " 'b317bec2-949e-445c-aa3e-4668f91c996b',\n",
              " '40f862b9-fa65-423b-948c-d1fc34b0d89b',\n",
              " '6e606867-885b-47f6-b627-0f2c93b5d2e3',\n",
              " 'a405ab9b-b6b9-41b0-8ebe-53f222968bdd',\n",
              " '09b0579f-2aea-4fa0-96cc-727718905284',\n",
              " '3fe0028a-b354-49ab-ae55-c0a9409729db',\n",
              " '44cbd52c-4393-41d7-81fd-e69affadf70c',\n",
              " '68c57b37-ee67-4e5e-9c39-8fb4c99d8bef',\n",
              " '21011aed-4754-4026-8c98-aabe79ea1ced',\n",
              " '0abad89b-e2e9-44b9-87eb-bc23c73ef4e4',\n",
              " '47c20192-3ee1-484a-a807-80a6a03f59b9',\n",
              " '72088728-4623-4d5e-a826-bf08593f90e8',\n",
              " '9ffec65b-69b4-4162-9ac7-e82424d340b2',\n",
              " 'a43fe220-3f52-4d81-90c9-aa0c4bed7155',\n",
              " '3d23cfe8-e11d-4ed8-8967-283a60738ba2',\n",
              " '944f5d5b-964e-4c65-9b09-a998e80a776c',\n",
              " '7cf9a57f-b9f6-4da8-a370-25d14d7a58f9',\n",
              " 'f62aff1b-1260-4840-a10a-c33da563fc6c',\n",
              " '65e14f1e-e29b-434a-80e2-ca026b0d8fb0',\n",
              " 'a248ba4e-e263-4702-aba6-f4cb40da09d7',\n",
              " 'bb2afa37-d1ac-4ebf-b73b-9905dd7007d6',\n",
              " 'f11d9337-14ad-4b0d-b7fb-0ce15613bff8',\n",
              " 'ec56feec-b7e9-460f-9518-e768d42b1b73',\n",
              " '3ea535fb-099d-4170-bf0f-700220cc952e',\n",
              " '60d3cb7c-d490-4454-8ee9-a07240781f02',\n",
              " 'd6426212-7a4b-4d92-bdd8-82fdc9782c69',\n",
              " '6198a771-f591-42eb-8366-8fd1c0913c34',\n",
              " 'd0523ae0-ab4d-4f2d-b587-0b956b364247',\n",
              " '8a66df93-3f69-4e13-bbcd-62142d8dbdda',\n",
              " '7a4027b5-b5f7-4b34-846c-26496eaca284',\n",
              " '581a7a43-d0a7-45d5-8d8a-43ba73c0c6a8',\n",
              " '10da2271-c03b-47cc-b5b0-efe76bd23975',\n",
              " '2528c76a-452a-4586-a723-82d71c8200d2',\n",
              " 'e5826967-9375-4f32-bdd7-5b71b7fbbf2b',\n",
              " 'e00df0d0-decf-495c-97aa-27e5672be818',\n",
              " '4c46bb73-32a3-483d-9b42-7990f4065ee4',\n",
              " '745be032-3bd0-4410-95da-c232ce65f0ea',\n",
              " '2514183d-0c38-4d83-8616-9008c9716138',\n",
              " 'd8eb4e4f-4d0d-4922-b12b-99be338121a4',\n",
              " '3f976879-3cd4-4be4-bf70-86b0ad207b08',\n",
              " '84dc8bcb-cd44-4d7b-8c31-16f7c84d7075',\n",
              " 'a72a3afa-47b3-40de-b55c-82b5c9717e47',\n",
              " '8713e9de-d04f-49aa-8ba2-a706d2660f0e',\n",
              " 'a9e5d4f2-57c1-4977-b497-8593475ebedf',\n",
              " '6731ff8a-b772-4cdd-8cb7-a2508418e479',\n",
              " 'c5f56355-8e45-449f-84f5-347fc664df39',\n",
              " '9d76de16-0478-4369-a388-e06fa4f2b312',\n",
              " 'b1d84177-d948-4016-b8f6-7856f6beb002',\n",
              " '1a931fc9-28cc-44ed-be26-0dc6011c602f',\n",
              " 'ae1ff651-b67b-437d-b396-fa6df549580a',\n",
              " '75242b9e-e7cf-474f-9473-b4f75359d58c',\n",
              " '32971b20-2f35-4a00-9f3b-2b550bcee35b',\n",
              " '11bb861b-3c1c-455a-96df-ff42b8ed4e2d',\n",
              " 'a6109f47-4617-47db-9514-9fae74d26ba9',\n",
              " '590bccc6-dd86-4a75-9aa0-9b36f511e968',\n",
              " '25835365-8c95-43f9-98eb-3a9ab53b0951',\n",
              " '111ef457-09b5-4b79-bdda-553a47a43f7a',\n",
              " 'ecbf0451-7ccc-452e-8c4e-2ab9f61312df',\n",
              " '2165bbe5-d8df-49b0-8c11-d9b83e238199',\n",
              " '53c4dff3-b636-4864-bd72-4a7983d877aa',\n",
              " 'e730ebeb-be80-45a3-b125-4e3b3dfe5f74',\n",
              " 'ae7a686b-8377-4f35-b2c1-f0e6d42c7f79',\n",
              " 'aeb821da-6b8e-4146-95a4-459125678770',\n",
              " '7191bcd8-691c-45b6-bacc-7b6878dde017',\n",
              " 'a0fa5973-6d6e-4d26-a06a-1db39c4200f2',\n",
              " 'a27ad796-7897-46b8-bcce-d72999390fea',\n",
              " 'a4a32f88-784e-40eb-a7c4-f86e11234396',\n",
              " '8e8e3f6b-d1d4-4a92-9ecf-6150344ef769',\n",
              " '8a00a625-72ba-4b70-8181-32f2566043c7',\n",
              " 'ae1757d6-90ea-4438-9804-4abc40b64268',\n",
              " '4b670075-f0c6-4757-8b05-6dd2f3a120e6',\n",
              " 'af2d4ca3-ad31-4c9d-9eb3-03ee524ce6b4',\n",
              " '71a4a277-bafd-406a-ab85-cb19d92c0a96',\n",
              " 'acf9419c-5934-4e75-85ab-4857d4d7561d',\n",
              " '4205ee36-80bb-4a43-a34b-f1a460c491cb',\n",
              " 'd353ef87-170d-46f6-a46e-59c361a79640',\n",
              " 'aee70727-0759-4222-ae46-dcd0d4f75628',\n",
              " '3cd7012e-74ee-49e2-9c93-832652dc7b54',\n",
              " '8e6909e1-cf5a-4dba-9a95-689dc67e8ac6',\n",
              " 'c032f4ff-3f59-40e6-a126-038c14cfb685',\n",
              " 'eff36f44-48a5-421b-a915-69d4d616d2bc',\n",
              " 'bf989411-765e-41bb-b7bb-fe061609d82b',\n",
              " '953bbc58-e1bd-476d-80e2-94249c0b5ef0',\n",
              " '6d17bb5d-cb2a-403c-a8a4-33a9c21b1ff0',\n",
              " 'd7e5bfd3-c798-44b8-890f-a5941b649687',\n",
              " '38b7b7f7-b2d2-4436-b9e1-eea00a97b61b',\n",
              " '6c5758fa-0fad-4dae-9b41-1421b59c8032',\n",
              " '2c21649e-b349-4c2e-8c3f-33317c73e061',\n",
              " '901666bd-0cbe-4c94-9f7c-ca5b7488a65d',\n",
              " '182fcef5-eab3-43e9-b31c-e33bbfd07981',\n",
              " 'cdb1ae9c-c8b4-444c-ad9d-5701e8f57be3',\n",
              " '185c1cbb-c05a-4927-ab2a-ded16bb72a9c',\n",
              " 'a633b188-975f-4d78-a4fa-1f6e46233ed5',\n",
              " '7138638a-6b69-4dc2-83a2-cae6b56aba90',\n",
              " '5487232a-79cd-4ff7-a017-4b4352fb9dda',\n",
              " 'c9654300-2b3d-46f1-ac41-a89ba24c931a',\n",
              " '1dd63c6d-04b8-4300-9531-1cc5e4047211',\n",
              " '81eea9c8-daa2-4fe5-9dc9-64d77636b880',\n",
              " 'adb20795-9d8d-49b0-8278-3b7d067ddb38',\n",
              " 'cf644b25-6c9c-4e7e-b7a3-dad670ecd8a3',\n",
              " '73c1b1e9-4ce8-40a8-99d0-d14c8b4c147d',\n",
              " 'a1b497e7-3fce-4ad8-b91d-afde2f22a393',\n",
              " 'a91f3bee-b739-4518-ad7e-b5fab2721336',\n",
              " 'fe4cf72b-9f8c-46fa-b8b1-da70c4b2a7a3',\n",
              " 'adcb5b6b-d212-4f47-9a79-b9e7c788432f',\n",
              " '960b9f3e-12fd-4c78-867e-bd53c1e5626b',\n",
              " 'e2951cad-6f86-4bfb-a006-b7d627c64828',\n",
              " '4e002f3f-831b-4607-99cd-410fb3dbc55d',\n",
              " '34d04b64-2a07-483a-a6fc-c1c592e94a15',\n",
              " '0287ed4f-ad82-4aa8-b851-778484a15e46',\n",
              " '083f9843-84bc-4d68-9503-9b1dbed1ccee',\n",
              " '5dd4b6cd-18a7-44b2-95bb-2426f080fe1a',\n",
              " 'ff41261e-aec3-4c05-9454-4c875a781f1a',\n",
              " '4aef9487-c5a9-47c8-861e-c9757db9f726',\n",
              " 'a47a339c-d804-4c01-811b-0d351a823270',\n",
              " 'e70c11ec-0068-4730-9093-73349aaf02ea',\n",
              " '10ed3cd0-07e4-400c-86a9-b8de145cd7af',\n",
              " '462675ad-b579-43f7-841a-9a54d19dddac',\n",
              " '12743d43-6440-4189-abcd-a60ea6750d16',\n",
              " '15d7911a-b154-4e81-80ed-2d2e03754090',\n",
              " '73cb9f62-0281-4e0e-a9b8-5fd5af31c9f5',\n",
              " '360d7b4a-480b-4f99-b26f-0cf900d4b6ce',\n",
              " '0773a1c5-28ff-4381-b9f0-e9882903cc29',\n",
              " '265d7155-74c6-433e-aa48-5f715491b9b0',\n",
              " '21926e1b-1e83-40a7-ae84-05a94bf482bd',\n",
              " 'a7f223ce-15bf-4c0c-9f66-7e57c9c35b09',\n",
              " '74728826-1b0f-4464-9431-952a63b2f745',\n",
              " '21bff1e7-495b-466b-afa1-600e149be690',\n",
              " 'ceeeac17-fe6d-45d3-8fd8-c02612bd7a99',\n",
              " '8a03524d-b352-4548-9247-c25b7fd67711',\n",
              " 'b8aa1be7-4024-4ff1-9ab2-b116a639f661',\n",
              " '0d3cfdfc-3323-4f06-8807-563acc54cd89',\n",
              " 'b3063c71-4c06-401d-a629-cde2e1caef47',\n",
              " '16aa5f35-08f3-4e79-9b8a-38e38c35df6a',\n",
              " '5061b222-0add-42c0-b1ed-033610e2bc54',\n",
              " 'd81a57d7-9f22-4d62-bd7a-78c46e1ba3cf',\n",
              " '9f3bbc32-4bf0-426c-ac48-002ee5b2c477',\n",
              " 'eb640ca2-ee44-4665-8452-d3547f79739f',\n",
              " 'eddcf454-eff5-4933-89d0-02fa07fbe209',\n",
              " 'c64e9861-f841-4c2f-9107-057382d0bef2',\n",
              " '93d18665-036f-4a2c-9541-c9e107f0643a',\n",
              " '087fe121-0635-46fa-bf10-eb5fd983dee2',\n",
              " 'ecdebea4-92b3-421b-9899-49705c26bfc6',\n",
              " '8c0746d6-cec9-494b-9503-385758e806ec',\n",
              " '13b74958-6f25-4174-b040-d15108688155',\n",
              " '51b1dea0-e492-4765-8476-7e86100d4e43',\n",
              " 'cf4272fa-78b4-4299-bef7-1cf1254b8cf4',\n",
              " '7fe08add-d75e-4c1f-9c0b-2c9e71719fb0',\n",
              " '1113186f-f4cd-432f-bd31-80aa15cd32c7',\n",
              " 'd2e0e486-6250-4aca-878a-5e132ea80d3a',\n",
              " '22243053-1ffe-4367-80e7-fae52f4d76fe',\n",
              " '1f93cbf5-4a0a-4a78-a85f-d346d180a758',\n",
              " 'f5fd3559-ca28-4e18-9f2e-9abba301f922',\n",
              " '6d6c1580-e776-4b27-9b43-c4448660372f',\n",
              " 'eeea0876-b352-48a5-96f3-2f02c5fee25d',\n",
              " 'cff2d7eb-5859-4523-972d-6a594a373a8e',\n",
              " '1644a40e-dffc-4efc-bdbe-17abe93d0de7',\n",
              " 'b14e7cf0-1968-4640-a56c-b1d0ffea7e3f',\n",
              " 'b15b4094-2e5f-410e-8839-aba1057dce71',\n",
              " 'a3e7ae58-5ae1-4e1b-89ab-04b5c54d8b11',\n",
              " '8ee023c0-cf40-46b0-8d74-82d9eb454da8',\n",
              " '7f27219d-1bdf-41ed-8562-e0f6fe1223cc',\n",
              " 'b86ec9a4-b436-4c9a-8e75-fb42e020ad41',\n",
              " 'fe20c2e0-27b8-4b18-b10f-a164b27dbd38',\n",
              " '6e01ec44-5728-4731-ad1c-a13cca2aad6b',\n",
              " 'cbcf0755-6dce-433d-91b6-84d219136dac',\n",
              " 'ad55722e-2669-4f7d-9f26-645997bf120d',\n",
              " '470ab165-0e41-408a-8f11-f1d2905ee12c',\n",
              " '835d988c-f05e-4cc4-ac45-73b759be36bb',\n",
              " '3dd19786-d247-442a-a530-3884869f29c1',\n",
              " '9b3f8494-7618-494a-aa91-a768c55a4d86',\n",
              " '29229478-bb23-43d5-b228-467ec171540f',\n",
              " 'bfa6a5d2-a247-4661-9830-3d5c233592ad',\n",
              " 'c43a1a28-fbeb-4c0f-b09d-b8e5f5d6720e',\n",
              " '5f7c1f80-3a94-431c-894b-a8778903b6bd',\n",
              " '4be96ae5-e8dd-4058-b61f-431cf21216a0',\n",
              " '1a33442e-60d0-4a7f-acdd-441cdc627e0e',\n",
              " '5661ab06-b661-4cde-ab3e-dd49a2a5d295',\n",
              " '14daec20-dbd5-4b21-b4d1-9871c5c36ab7',\n",
              " 'abef7b21-cc4d-4a99-9ae0-6886962d4f7e',\n",
              " '6d6bd611-d2a0-4ded-bf96-7cc00bf98aea',\n",
              " '6958a8ff-f56b-4c4b-ba2f-4da5679a6cc9',\n",
              " '012c2b2f-cd77-4e48-adab-1de0b51d4a28',\n",
              " '02cc9149-16bb-4484-a1b1-1c1c8ff3be26',\n",
              " '2b2f8b73-535d-423e-9b1e-becd7f264d66',\n",
              " '1fc99e91-7cc8-43cf-a348-c2985becbc5a',\n",
              " 'ad8ccef7-b8b5-431a-8e15-5cb9103a7bb7',\n",
              " '879bb675-aa36-4471-aabe-f1c146591ba0',\n",
              " '15cab48f-3e2a-423a-911a-f91dccf9cf2a',\n",
              " 'f3666ede-185e-43fe-b82f-1e1b42910397',\n",
              " '5bb3537e-1a85-42a1-8148-788014a8e93a',\n",
              " '1e2503c7-7f16-413d-b1f0-e377c67dc24d',\n",
              " 'd0b54b98-f53f-4a0c-ae76-81334041f110',\n",
              " '65592f63-808f-47a0-88c3-b57f0ca5c2db',\n",
              " '3fac1454-ee6c-47e0-bfc8-11be79775b1c',\n",
              " '1c40d910-4f4d-4c2e-a5b8-2ad67a449f49',\n",
              " '9a1cc477-5641-4f74-982a-3eb607fc895c',\n",
              " '19952cff-acf6-4f59-9c8a-8ae6287423f9',\n",
              " 'e6558bbf-3f79-4d75-9b3f-cd295e95645c',\n",
              " '26aebf5f-1767-4fdd-9e9f-c83f58b1cf5c',\n",
              " '5fb648bb-3e41-4f7d-9c4c-9457a0626d2d',\n",
              " '322b73c3-010c-470a-a4c5-5a81a7a91b79',\n",
              " '0760d322-8cc1-4fd7-b26b-7008a7afbcce',\n",
              " '9484abb9-0aac-4b08-bcf3-ab5a33fc8d3e',\n",
              " '458a08b4-5ed2-4cd8-8346-8ed564bc1573',\n",
              " 'b12ed1c1-2910-4646-8048-bcede0eb3a05',\n",
              " '4115af41-3fd8-4863-8650-be4d164754cf',\n",
              " 'b7a42a19-0c14-461e-ba13-6919c7cc6f19',\n",
              " '8a1b5e10-a6b3-4b15-bdac-62aae49e0cb0',\n",
              " '2f7f297d-b355-42da-90be-87e7f15103c7',\n",
              " 'fc47a349-6eb1-4dff-b74c-aef136ab8006',\n",
              " '4caa837e-0d66-45e1-a7e3-5d3a9eff3f8e',\n",
              " 'dd28b1b1-2ef3-4594-b9c3-7e26563f5945',\n",
              " '40825472-81f4-4489-94de-383d7324e367',\n",
              " '01f85eb1-0787-45e4-853f-0ccf4b6fbc9a',\n",
              " '83884e68-5369-4ebc-a9db-1effe840274f',\n",
              " '8be7954e-c90a-4cbd-be9c-c447ac892357',\n",
              " 'a947bb6a-fcbd-44de-913e-e2cd395f3bcd',\n",
              " 'c323c7ed-36d1-4931-9474-45a3bd0e110d',\n",
              " '7900c26c-05c7-49a3-adcd-67917ca4e005',\n",
              " 'f0bd0c3c-a687-466a-be1c-2cc86200a43a',\n",
              " '0dd02483-e0c9-4134-8c1f-d4e39f776668',\n",
              " '22af2945-988e-4a9b-8479-fea37a1d80bd',\n",
              " 'de65fff8-3241-48fa-a1e1-ee392e03a977',\n",
              " '56e39ebd-966f-40e4-b058-4bfb8837dea6',\n",
              " '1021ea5e-68fc-4b55-bf29-2522763a2095',\n",
              " 'cbc5d7f7-d838-4887-b2a9-2b2fbf1d4218',\n",
              " 'f33c89cf-5b6b-4502-a0a9-8365f75bc5c5',\n",
              " '8ba68a77-8aee-4d01-937c-fe05ed4dd94d',\n",
              " '92249e2e-fba5-4bad-90e3-821d3b9994ff',\n",
              " '300d4491-29f4-44e7-96f3-b05c34c05067',\n",
              " 'b918e258-7d19-4b8d-b2a3-5ab77d747454',\n",
              " '7a9ba23c-3b38-46e7-8d4e-a0d52499318a',\n",
              " 'a1a73f95-51ef-4b31-a891-363e712619cb',\n",
              " '233fdb11-5813-491e-86be-02ea39555d82',\n",
              " 'cb7e2956-7382-4ffe-88d4-be77307dd315',\n",
              " '7588bedf-f87b-4231-9a62-7d576129c26b',\n",
              " '283c139c-a4f1-4585-b96b-791f5b0f3b2e',\n",
              " '316d8722-b42d-43d9-8da8-1ca07d4b68c8',\n",
              " '5df25484-6b0c-4ca8-b952-1fa67186aa98',\n",
              " 'cc9a99f8-6e0d-4d9c-b4a5-b60add306c88',\n",
              " 'a3d66e29-dab3-46f4-9000-3dd44d2d02ba',\n",
              " '9526cb90-2342-481f-aa79-12db21593aae',\n",
              " '02991b4a-f669-41c7-910e-74ca481bed77',\n",
              " '11fc184a-df60-4575-8388-9af9237304b4',\n",
              " '091eb259-9f96-4dbb-9500-92b40882122f',\n",
              " '3a8a74f1-5798-4e53-805e-18e36ad05ac1',\n",
              " 'ae6198f6-e396-46e9-b007-e14054af2b73',\n",
              " 'c029cbb0-1ade-4d77-8788-2642799e2306',\n",
              " '91f8dc60-efa3-474c-8baf-a2316b61ab0a',\n",
              " '7328a68e-b9c0-4837-8e7d-8db4c82083b0',\n",
              " 'd9923095-153e-452d-8386-72667dd35579',\n",
              " '905c3cb7-bc0d-4d67-b945-c5dce169c1f0',\n",
              " '7d32599a-006b-4cf8-99fb-8aaa08e26fce',\n",
              " '74c5d6b6-ddad-4575-9ffe-e04b25448f60',\n",
              " 'acbd21a7-16c1-40b3-9e82-9fa57ee1dba1',\n",
              " '37827717-54e3-493d-963a-4b4c6f7bcc01',\n",
              " '7c68eeb0-c110-472c-a203-5554083e084e',\n",
              " '31492deb-8a2e-47eb-aaaa-26d639baac3c',\n",
              " '88ca1168-ff19-4d0c-80e8-7d0711256c88',\n",
              " 'bd5a2e47-7ca5-4624-93ca-95f70c778f22',\n",
              " '9b6e9ed5-3e65-4de4-8f78-ceebbd349a0c',\n",
              " 'd217df30-d2dd-4549-bc99-abd9b11d74ee',\n",
              " 'fd6bd7de-f52e-4cbd-b73e-8bac65415344',\n",
              " '02c5b28e-89e9-4fed-b9e8-eaaefde44d44',\n",
              " '56a864d9-82d4-4900-972a-422d1c91803a',\n",
              " '505c0b34-5549-42fa-aff0-b139edd9ffc8',\n",
              " 'cc3a0e20-c7e9-4080-bdd6-faf0977db8ee',\n",
              " 'b1be37af-7121-47f5-9b4e-365fd6e46861',\n",
              " '40873bdf-2236-4a7a-9f26-4997954d33bd',\n",
              " '70193137-a969-4a97-b962-f95330ddf87c',\n",
              " '05a2f5e5-1cc8-4be4-945d-950b5a5d631c',\n",
              " '9edef2ce-e81b-425a-9a7c-1485c13b6412',\n",
              " '10c51cc3-c3f2-49fa-acca-7a37ebdebc0e',\n",
              " '07ac0e1a-7363-4bd5-8142-a68a43b2c6ff',\n",
              " 'a2f0fa4a-59cf-4c6d-8302-9fddc93618a3',\n",
              " 'bf6588ce-1834-4286-898c-b5665c477fb6',\n",
              " 'b6974e89-845a-4ee7-a8f1-06f8b556d9ce',\n",
              " 'abc33ef1-c1e2-4efc-8a89-1bd7a0089071',\n",
              " 'b1a678cc-7af3-403a-81bc-5dca6111a638',\n",
              " 'a2b5e696-b07d-42be-ad1e-9282568dcb28',\n",
              " '216a1a55-5f86-4654-98c4-3b6a5329a222',\n",
              " '292fabd3-fd05-4c02-a6a2-ef1511bd17df',\n",
              " '1eea7766-165f-41c0-bffc-79ddb3a1ac4d',\n",
              " '85b6313b-a35e-457b-80ff-040601f6a182',\n",
              " '80962012-a95e-4add-8dfa-0cfa3302455d',\n",
              " 'd6000213-019d-437d-a3e4-2a732e7ad433',\n",
              " 'de166a45-23e5-4a77-826a-bfbee648dbfe',\n",
              " 'cac85d5a-65e7-4643-bc47-1da325928fb3',\n",
              " 'dfffd7f5-f82d-480c-ae8b-6b3c192e1b5c',\n",
              " '9d63e377-45ca-45e3-b04f-1026bb2cbeb7',\n",
              " 'fe357568-9cf2-44cd-affe-6a45c9919d44',\n",
              " '3dbb2c28-e73b-4e54-9a87-c14a92e4e677',\n",
              " 'a5c986b7-7f17-4af3-b4e7-b85075efb962',\n",
              " '7a391609-374e-40c3-9181-2d2d02e74640',\n",
              " '744127af-8480-42d2-98aa-ec315821d080',\n",
              " 'a37a2c90-caaa-439e-b9b3-d0212c4c7c7b',\n",
              " '4c2e377e-fb8c-4a37-b8bf-03aca348f59d',\n",
              " '3226f03d-f5ac-4aff-b2c1-d16409d22918',\n",
              " '92586dd4-09f3-4bc6-b799-5606134fa6fd',\n",
              " '18d4e667-f0b6-4472-813d-7699b6e082b8',\n",
              " '57e3d9f0-1bf4-49d3-9379-2691b9ef4b58',\n",
              " '73b612a4-263f-4136-8886-5446d8d0cf88',\n",
              " '7e6052a0-aa79-423b-89bb-39f284d23282',\n",
              " '3b1f6f73-e12d-4181-8635-1b0a7c9c3360',\n",
              " 'b944c9f8-d457-430a-955b-9f7fddb28c0d',\n",
              " '61053a18-54c9-4cbd-a3f0-dd70135efd25',\n",
              " 'd9d14702-4389-403c-bb3c-f3bff74d47eb',\n",
              " 'be4878f0-9180-486b-bc66-768f6e22cab7',\n",
              " '8d87a750-82fc-45b4-9a10-b45a59932077',\n",
              " 'f39c0dbe-d689-4ec0-8ffd-f0749f68fc80',\n",
              " 'c859d06b-95e8-4612-a865-44e83485878e',\n",
              " 'a4cbc40e-b3c3-4594-8171-175b6ffc9f30',\n",
              " '6b8d1232-1425-4e2e-9d41-9681cc717a5e',\n",
              " '6b8f92ae-65d6-4bb1-b8fd-7c52429f7ca2',\n",
              " '3a9a3671-de3f-4114-a9ab-18159aba905a',\n",
              " '428a3522-faa5-4f8c-aa53-b7e16f9d0b99',\n",
              " '8630088b-bf24-4a3f-be12-e7b582a4c137',\n",
              " 'f5c4c471-201d-4ae4-bba4-d9895325e99e',\n",
              " '27cd028a-ab28-4913-89aa-efa429bfab83',\n",
              " '7328a8b5-f0d0-4f30-b448-a20a840a1518',\n",
              " '98e8b2cf-473d-47c9-a220-e0f6a7eb0f6f',\n",
              " '46e99b36-5425-441d-aebd-b3bf53677aa2',\n",
              " 'fbfa0254-b285-4d63-a950-b5439c102934',\n",
              " '4c92358b-ac68-4aea-9e4f-74375cc88f25',\n",
              " '0e29ee5a-18d2-43e9-b834-6cdcb50adbaa',\n",
              " '36b2d8b4-cb63-4f16-95a2-61f018a19ec4',\n",
              " '697f052b-4bcc-4696-b848-0236892edf7a',\n",
              " '03852808-3599-4999-ad99-bac50eb9e74c',\n",
              " 'e48045d9-2854-4947-9411-f884638e6cad',\n",
              " 'cced5a7d-1457-46cf-b74f-3ada0713412a',\n",
              " 'f2721847-5ce5-4d84-80e5-30a96935eb24',\n",
              " 'b0ce7b05-f17d-4dbd-81dc-683e9662cf9c',\n",
              " 'ee8f4408-e7f3-4ceb-ba77-90a50570b5d5',\n",
              " '59704f67-1062-44db-b26b-c46fa66b7d99',\n",
              " 'c044f61c-d5b2-4e7f-90a6-b1ce2750e25d',\n",
              " 'b2dfdb36-a370-438c-8b0c-c5549a1b025e',\n",
              " '790a84ad-d39d-435c-ae15-93ca378c8543',\n",
              " '48fcc8fe-8004-4121-9e74-2349f6929588',\n",
              " 'c0f7bbfb-2e60-461a-bd52-cea41dfc11c7',\n",
              " 'd124d3ad-8c01-4750-9387-c786a111f551',\n",
              " '19964366-87e1-4fea-8550-5db9e034d9f0',\n",
              " '070d7420-13b6-470d-bf73-50a3951129d8',\n",
              " 'b9891547-a195-4de7-b073-996b60ebf843',\n",
              " 'ac3ef85b-c2b2-4235-9749-e10bcb2bb7a2',\n",
              " '142b4508-26b5-4c51-bee8-2ac9f556da3f',\n",
              " '71a4985c-997d-42a4-aaf9-e483ab7c756e',\n",
              " '34a93e3c-a38d-4476-a4d7-bb562ee2c76b',\n",
              " '332c8a70-baea-4e5a-acf3-a482402724e2',\n",
              " '39e2f548-6fca-41e9-8124-70c5f0da8b6b',\n",
              " 'df9ba06b-d4e7-4cbe-89ae-79bb45759f9a',\n",
              " '83c078c4-f541-48e5-b792-eeb10a5645dc',\n",
              " '68060950-f9e9-4ab0-9e38-23c3257f8693',\n",
              " '71314a1c-afdd-45c0-9744-c3faead1d6e9',\n",
              " '630f47c7-ad1c-447e-8b58-ecfd6d8da4bc',\n",
              " '452c176e-cf38-46ab-b1e6-4aeff78e75d3',\n",
              " 'de3d007c-de21-49b5-ae34-f8ade9c92aed',\n",
              " 'a44b92ab-3ba6-4baa-81ab-177c867ecafd',\n",
              " '523f69c9-af25-4351-873c-d55e0ceca3a4',\n",
              " '9d9f2735-8b7b-4e51-9985-11c0b6c604e1',\n",
              " '7c8a7021-9e8f-4d51-94f9-63534b5cd735',\n",
              " 'b8f590bb-bb49-4c58-af33-61de6a9fdcbc',\n",
              " '4798d0bb-36da-4bc8-ae73-54054ce333ad',\n",
              " 'bab30b16-9a72-4c79-baa3-7784cebaaa8b',\n",
              " '11cceeb7-af06-4535-a779-7b3f61ab7bff',\n",
              " '477a55a8-17ea-463c-8928-74655698be8f',\n",
              " '970ee6f2-4c9f-4ad9-9ae1-0f00cd8e7206',\n",
              " '6b373f11-6365-422f-bc60-5642d2bf0c72',\n",
              " '8788bf64-6624-4029-9fad-d3487262ec78',\n",
              " '0a5bccb8-e147-4031-a4df-e445028bfe64',\n",
              " 'e8e462ac-bcb6-4995-96c4-e41dad67543f',\n",
              " '2aed0565-3e72-4edd-9b0e-073be89ecc0e',\n",
              " '20817aea-50f6-4fcc-936c-ccdbf4ad666a',\n",
              " 'fec74796-db07-45b4-8443-48b9abc12a92',\n",
              " '6c922a70-0035-450a-b474-703d97d4a317',\n",
              " '013bb68d-c0ce-4e17-a2af-688f80485360',\n",
              " '7a21d82c-38f4-450e-ab63-2937c0507211',\n",
              " 'f896e84c-a8b1-41cd-99bb-2b432d59fc75',\n",
              " 'abac3d47-f250-4f5e-abfa-6a46cdc4586e',\n",
              " '02f7d4f9-e9b6-4736-b756-9cb6543e3bdb',\n",
              " '14f1fe0d-6b25-49ff-8426-fbf1f8630f47',\n",
              " '6a760f5d-f1c2-4c3f-8964-47e765f1c65c',\n",
              " '999cbc5c-0134-4c38-8172-378ef52e57b8',\n",
              " '0c5798af-1bd8-4474-9be7-24927c4f14fe',\n",
              " 'ea2b5a9c-5474-4cf0-b39a-a26087c3f93c',\n",
              " '8c13f4c0-77e7-4687-b71b-ac4d60d03b9e',\n",
              " '987638c3-6a12-4327-aae9-490f84edd8d6',\n",
              " '10fa8336-7c73-4fbf-b51c-dff37c4445c1',\n",
              " 'd4166c8d-382b-4a86-b26b-0f78e34238d0',\n",
              " 'dc0d9c13-0f76-4f90-8b83-fe7bab0d846f',\n",
              " 'ac090fc0-7ade-4499-b091-ded7fcc8d18a',\n",
              " '19caf919-2b5e-41f2-84f5-8f0bf1a0f4a5',\n",
              " 'ca464906-3c1a-409f-8afb-cf38569ca01c',\n",
              " '1a46d68b-3d38-4274-841a-9e168ae73664',\n",
              " 'ade246d7-b8d1-4f06-8758-3b0c524de425',\n",
              " '5866d352-25d5-4c55-abe3-5a61393c2c56',\n",
              " '17aaf845-4e75-47b3-af99-7eb1b2b29a86',\n",
              " '2dc632af-f48b-4c0c-a1fe-2bd9212fc434',\n",
              " 'e42c1087-7cbe-48a0-a4e9-9051e59561af',\n",
              " '819f961a-e94c-468f-b005-0b2b765236a8',\n",
              " '3f416949-0d72-446f-a616-dc56acd4a205',\n",
              " 'ec09d397-6d6e-45c1-926d-bd96774be804',\n",
              " '68b6dac6-2057-4fb1-b30c-2dda10d8115c',\n",
              " 'be0976f8-cc32-460c-af30-086928a160a9',\n",
              " '9af67c7e-69e5-4706-991e-d2fa46f5f8ef']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.add(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d574a062-1900-4b74-be9a-6248ffb8bbbe",
      "metadata": {
        "id": "d574a062-1900-4b74-be9a-6248ffb8bbbe"
      },
      "source": [
        "## Build Retrieval Pipeline from Scratch\n",
        "\n",
        "We show how to build a retrieval pipeline. Similar to ingestion, we fast-track the steps. Take a look at our [retrieval guide](https://gpt-index.readthedocs.io/en/latest/examples/low_level/retrieval.html) for more details!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cd7a7465-6a3a-4379-8ac9-1dad7d8441e2",
      "metadata": {
        "id": "cd7a7465-6a3a-4379-8ac9-1dad7d8441e2"
      },
      "outputs": [],
      "source": [
        "query_str = \"\"\"I have so many issues to address. I have a history of sexual abuse, Im a breast cancer survivor and I am a lifetime insomniac. \n",
        "    I have a long history of depression and Im beginning to have anxiety. I have low self esteem but Ive been happily married for almost 35 years. \n",
        "    Ive never had counseling about any of this. Do I have too many issues to address in counseling?\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b076aa-4ebb-4f26-8b31-387a01a47405",
      "metadata": {
        "id": "20b076aa-4ebb-4f26-8b31-387a01a47405"
      },
      "source": [
        "### 1. Generate a Query Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "159bda45-beb5-48bc-bf33-b9d1c44188b9",
      "metadata": {
        "id": "159bda45-beb5-48bc-bf33-b9d1c44188b9"
      },
      "outputs": [],
      "source": [
        "query_embedding = embed_model.get_query_embedding(query_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2c8594-bc95-41b0-a0bb-35b4f02a734f",
      "metadata": {
        "id": "cf2c8594-bc95-41b0-a0bb-35b4f02a734f"
      },
      "source": [
        "### 2. Query the Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b5c62ffc-2092-4fd0-85c4-acde0b6c3b4f",
      "metadata": {
        "id": "b5c62ffc-2092-4fd0-85c4-acde0b6c3b4f"
      },
      "outputs": [],
      "source": [
        "# construct vector store query\n",
        "from llama_index.core.vector_stores import VectorStoreQuery\n",
        "\n",
        "query_mode = \"default\"\n",
        "# query_mode = \"sparse\"\n",
        "# query_mode = \"hybrid\"\n",
        "\n",
        "vector_store_query = VectorStoreQuery(\n",
        "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "06051002-db44-404b-a946-2b37b3b6ca67",
      "metadata": {
        "id": "06051002-db44-404b-a946-2b37b3b6ca67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'question': 'I stress over everything. If I don\\'t have enough \"quality time\" with my boyfriend, I start to feel resentment towards him. He has three children, and they are great kids, but I find we don\\'t have much time together. I break down easily and find myself depressed.', 'answer': 'Everyone has some level of anxiety - it\\'s what helps us respond to stressors in our lives and clues us into the fact that we need to respond to something going on. However, if you\\'re feeling overwhelmed by racing thoughts, feeling like you spend a lot of energy worrying about something specific or even pretty much anything at all, and you\\'re starting to find that it\\'s getting in your way when it comes to living your life the way you want, then I\\'d suggest seeing a counselor or therapist for an assessment for anxiety.\\xa0Your other concerns, though, seem pretty \"normal\" for someone who is in a relationship with a partner who has children. As a married stepmother, I\\'ve been there, and as a therapist, I can tell you that the boundaries and communication skills you have in your relationships need to be healthy for you to feel healthy. Finding some support from a group of others whose partners have children might be really helpful - being able to hear from others that you aren\\'t alone in feeling like this can really lighten the load, and they might have some good ideas for handling certain situations. Another option is to find a counselor or therapist who has experience working with step/blended family dynamics - because even if you\\'re not officially married, those dynamics come into play whenever there are kids from another relationship involved. You may even consider couples counseling so that both of you can learn to talk about your relationship needs and concerns in a way that will encourage connection and strengthen your sense of partnership.', 'topic': 'depression'}\n"
          ]
        }
      ],
      "source": [
        "# returns a VectorStoreQueryResult\n",
        "query_result = vector_store.query(vector_store_query)\n",
        "print(query_result.nodes[0].get_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff153ee-5774-4374-98a6-038775fb1d6a",
      "metadata": {
        "id": "dff153ee-5774-4374-98a6-038775fb1d6a"
      },
      "source": [
        "### 3. Parse Result into a Set of Nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9efda092-69ee-404e-8667-28e866c0e4d1",
      "metadata": {
        "id": "9efda092-69ee-404e-8667-28e866c0e4d1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import NodeWithScore\n",
        "from typing import Optional\n",
        "\n",
        "nodes_with_scores = []\n",
        "for index, node in enumerate(query_result.nodes):\n",
        "    score: Optional[float] = None\n",
        "    if query_result.similarities is not None:\n",
        "        score = query_result.similarities[index]\n",
        "    nodes_with_scores.append(NodeWithScore(node=node, score=score))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445ee65d-bd12-46e5-817d-e21d97718338",
      "metadata": {
        "id": "445ee65d-bd12-46e5-817d-e21d97718338"
      },
      "source": [
        "### 4. Put into a Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f817dbf4-926c-4aa2-a3b6-946c45df0893",
      "metadata": {
        "id": "f817dbf4-926c-4aa2-a3b6-946c45df0893"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import QueryBundle\n",
        "from llama_index.core.retrievers import BaseRetriever\n",
        "from typing import Any, List\n",
        "\n",
        "\n",
        "class VectorDBRetriever(BaseRetriever):\n",
        "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_store: ElasticsearchStore,\n",
        "        embed_model: Any,\n",
        "        query_mode: str = \"default\",\n",
        "        similarity_top_k: int = 2,\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "        self._vector_store = vector_store\n",
        "        self._embed_model = embed_model\n",
        "        self._query_mode = query_mode\n",
        "        self._similarity_top_k = similarity_top_k\n",
        "        super().__init__()\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        \"\"\"Retrieve.\"\"\"\n",
        "        query_embedding = embed_model.get_query_embedding(\n",
        "            query_bundle.query_str\n",
        "        )\n",
        "        vector_store_query = VectorStoreQuery(\n",
        "            query_embedding=query_embedding,\n",
        "            similarity_top_k=self._similarity_top_k,\n",
        "            mode=self._query_mode,\n",
        "        )\n",
        "        query_result = vector_store.query(vector_store_query)\n",
        "\n",
        "        nodes_with_scores = []\n",
        "        for index, node in enumerate(query_result.nodes):\n",
        "            score: Optional[float] = None\n",
        "            if query_result.similarities is not None:\n",
        "                score = query_result.similarities[index]\n",
        "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
        "\n",
        "        return nodes_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6dbaf309-a3fb-4d01-bc7b-4efab92e6e3d",
      "metadata": {
        "id": "6dbaf309-a3fb-4d01-bc7b-4efab92e6e3d"
      },
      "outputs": [],
      "source": [
        "retriever = VectorDBRetriever(\n",
        "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "977c28a6-065a-408a-b007-e611d2d99153",
      "metadata": {
        "id": "977c28a6-065a-408a-b007-e611d2d99153"
      },
      "source": [
        "## Plug this into our RetrieverQueryEngine to synthesize a response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ae459e87-daff-433d-a2bd-fd4a934357ef",
      "metadata": {
        "id": "ae459e87-daff-433d-a2bd-fd4a934357ef"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "312bdaba-6a91-4601-96f9-c64d1ae09007",
      "metadata": {
        "id": "312bdaba-6a91-4601-96f9-c64d1ae09007"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      22.89 ms /   237 runs   (    0.10 ms per token, 10352.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =   31449.02 ms /   913 tokens (   34.45 ms per token,    29.03 tokens per second)\n",
            "llama_print_timings:        eval time =   34101.34 ms /   236 runs   (  144.50 ms per token,     6.92 tokens per second)\n",
            "llama_print_timings:       total time =   65948.91 ms /  1149 tokens\n"
          ]
        }
      ],
      "source": [
        "query_str = \"\"\"I have so many issues to address. I have a history of sexual abuse, Im a breast cancer survivor and I am a lifetime insomniac. \n",
        "    I have a long history of depression and Im beginning to have anxiety. I have low self esteem but Ive been happily married for almost 35 years. \n",
        "    Ive never had counseling about any of this. Do I have too many issues to address in counseling?\n",
        "    \"\"\"\n",
        "\n",
        "response = query_engine.query(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "72f1096f-96b6-4a90-8524-b9ebe4532661",
      "metadata": {
        "id": "72f1096f-96b6-4a90-8524-b9ebe4532661",
        "outputId": "bbda5898-1c43-4610-e8c8-4e966d025745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " No, you don't have too many issues to address in counseling. It's completely normal and healthy to want to work through your past experiences, mental health struggles, and relationship concerns with a trained therapist or counselor. In fact, seeking professional help is often the first step towards healing and growth.\n",
            "It's important to remember that you don't have to go through any of this alone. A therapist can provide a safe and supportive space for you to process your experiences and emotions, and offer tools and strategies to help you manage your mental health and relationships.\n",
            "Additionally, it's important to find a therapist who is experienced in working with survivors of sexual abuse and individuals with low self-esteem. They can provide specialized support and guidance to help you work through these issues.\n",
            "Overall, don't hesitate to seek professional help if you feel like you're struggling to cope with your experiences or mental health. It's okay to ask for help, and it's important to prioritize your well-being.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "f7838289-5a59-4042-b4b7-037f66d99be4",
      "metadata": {
        "id": "f7838289-5a59-4042-b4b7-037f66d99be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'question': 'I stress over everything. If I don\\'t have enough \"quality time\" with my boyfriend, I start to feel resentment towards him. He has three children, and they are great kids, but I find we don\\'t have much time together. I break down easily and find myself depressed.', 'answer': 'Everyone has some level of anxiety - it\\'s what helps us respond to stressors in our lives and clues us into the fact that we need to respond to something going on. However, if you\\'re feeling overwhelmed by racing thoughts, feeling like you spend a lot of energy worrying about something specific or even pretty much anything at all, and you\\'re starting to find that it\\'s getting in your way when it comes to living your life the way you want, then I\\'d suggest seeing a counselor or therapist for an assessment for anxiety.\\xa0Your other concerns, though, seem pretty \"normal\" for someone who is in a relationship with a partner who has children. As a married stepmother, I\\'ve been there, and as a therapist, I can tell you that the boundaries and communication skills you have in your relationships need to be healthy for you to feel healthy. Finding some support from a group of others whose partners have children might be really helpful - being able to hear from others that you aren\\'t alone in feeling like this can really lighten the load, and they might have some good ideas for handling certain situations. Another option is to find a counselor or therapist who has experience working with step/blended family dynamics - because even if you\\'re not officially married, those dynamics come into play whenever there are kids from another relationship involved. You may even consider couples counseling so that both of you can learn to talk about your relationship needs and concerns in a way that will encourage connection and strengthen your sense of partnership.', 'topic': 'depression'}\n"
          ]
        }
      ],
      "source": [
        "print(response.source_nodes[0].get_content())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66390db6",
      "metadata": {},
      "source": [
        "## Get output of all test questions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "465e52c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      34.19 ms /   350 runs   (    0.10 ms per token, 10236.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =  112741.02 ms /     2 tokens (56370.51 ms per token,     0.02 tokens per second)\n",
            "llama_print_timings:        eval time =  108006.37 ms /   349 runs   (  309.47 ms per token,     3.23 tokens per second)\n",
            "llama_print_timings:       total time =  114315.10 ms /   351 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      53.47 ms /   416 runs   (    0.13 ms per token,  7780.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   54146.47 ms /  1157 tokens (   46.80 ms per token,    21.37 tokens per second)\n",
            "llama_print_timings:        eval time =  369499.77 ms /   415 runs   (  890.36 ms per token,     1.12 tokens per second)\n",
            "llama_print_timings:       total time =  424834.27 ms /  1572 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      59.87 ms /   512 runs   (    0.12 ms per token,  8551.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =   36736.64 ms /   733 tokens (   50.12 ms per token,    19.95 tokens per second)\n",
            "llama_print_timings:        eval time =  141932.67 ms /   511 runs   (  277.75 ms per token,     3.60 tokens per second)\n",
            "llama_print_timings:       total time =  179868.94 ms /  1244 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      47.52 ms /   382 runs   (    0.12 ms per token,  8039.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =   35662.08 ms /   953 tokens (   37.42 ms per token,    26.72 tokens per second)\n",
            "llama_print_timings:        eval time =  216309.77 ms /   381 runs   (  567.74 ms per token,     1.76 tokens per second)\n",
            "llama_print_timings:       total time =  253062.12 ms /  1334 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      53.66 ms /   427 runs   (    0.13 ms per token,  7957.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =   32355.45 ms /   668 tokens (   48.44 ms per token,    20.65 tokens per second)\n",
            "llama_print_timings:        eval time =  215743.58 ms /   426 runs   (  506.44 ms per token,     1.97 tokens per second)\n",
            "llama_print_timings:       total time =  249258.42 ms /  1094 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      35.13 ms /   316 runs   (    0.11 ms per token,  8995.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =   30504.81 ms /   699 tokens (   43.64 ms per token,    22.91 tokens per second)\n",
            "llama_print_timings:        eval time =  142151.50 ms /   315 runs   (  451.27 ms per token,     2.22 tokens per second)\n",
            "llama_print_timings:       total time =  173460.79 ms /  1014 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      14.72 ms /   163 runs   (    0.09 ms per token, 11075.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21414.79 ms /   717 tokens (   29.87 ms per token,    33.48 tokens per second)\n",
            "llama_print_timings:        eval time =   19216.59 ms /   162 runs   (  118.62 ms per token,     8.43 tokens per second)\n",
            "llama_print_timings:       total time =   41035.25 ms /   879 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      32.31 ms /   329 runs   (    0.10 ms per token, 10181.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19807.54 ms /   791 tokens (   25.04 ms per token,    39.93 tokens per second)\n",
            "llama_print_timings:        eval time =   41479.62 ms /   328 runs   (  126.46 ms per token,     7.91 tokens per second)\n",
            "llama_print_timings:       total time =   61991.36 ms /  1119 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      38.23 ms /   389 runs   (    0.10 ms per token, 10176.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18823.67 ms /   778 tokens (   24.19 ms per token,    41.33 tokens per second)\n",
            "llama_print_timings:        eval time =   49037.52 ms /   388 runs   (  126.39 ms per token,     7.91 tokens per second)\n",
            "llama_print_timings:       total time =   68634.56 ms /  1166 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.91 ms /   273 runs   (    0.10 ms per token,  9782.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16065.40 ms /   631 tokens (   25.46 ms per token,    39.28 tokens per second)\n",
            "llama_print_timings:        eval time =   32729.05 ms /   272 runs   (  120.33 ms per token,     8.31 tokens per second)\n",
            "llama_print_timings:       total time =   49347.66 ms /   903 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      34.56 ms /   358 runs   (    0.10 ms per token, 10357.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23278.11 ms /   973 tokens (   23.92 ms per token,    41.80 tokens per second)\n",
            "llama_print_timings:        eval time =   44497.70 ms /   357 runs   (  124.64 ms per token,     8.02 tokens per second)\n",
            "llama_print_timings:       total time =   68578.22 ms /  1330 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      38.84 ms /   416 runs   (    0.09 ms per token, 10710.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =   27774.50 ms /  1077 tokens (   25.79 ms per token,    38.78 tokens per second)\n",
            "llama_print_timings:        eval time =   51215.10 ms /   415 runs   (  123.41 ms per token,     8.10 tokens per second)\n",
            "llama_print_timings:       total time =   79871.27 ms /  1492 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      33.34 ms /   362 runs   (    0.09 ms per token, 10856.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =   38957.00 ms /  1545 tokens (   25.21 ms per token,    39.66 tokens per second)\n",
            "llama_print_timings:        eval time =   49448.12 ms /   361 runs   (  136.98 ms per token,     7.30 tokens per second)\n",
            "llama_print_timings:       total time =   89067.64 ms /  1906 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      35.87 ms /   392 runs   (    0.09 ms per token, 10928.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =   32831.97 ms /  1244 tokens (   26.39 ms per token,    37.89 tokens per second)\n",
            "llama_print_timings:        eval time =   50531.80 ms /   391 runs   (  129.24 ms per token,     7.74 tokens per second)\n",
            "llama_print_timings:       total time =   84112.61 ms /  1635 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      48.94 ms /   512 runs   (    0.10 ms per token, 10462.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25252.76 ms /   941 tokens (   26.84 ms per token,    37.26 tokens per second)\n",
            "llama_print_timings:        eval time =   64965.15 ms /   511 runs   (  127.13 ms per token,     7.87 tokens per second)\n",
            "llama_print_timings:       total time =   91187.43 ms /  1452 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      45.13 ms /   471 runs   (    0.10 ms per token, 10436.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21185.85 ms /   846 tokens (   25.04 ms per token,    39.93 tokens per second)\n",
            "llama_print_timings:        eval time =   57398.14 ms /   470 runs   (  122.12 ms per token,     8.19 tokens per second)\n",
            "llama_print_timings:       total time =   79470.08 ms /  1316 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      13.23 ms /   143 runs   (    0.09 ms per token, 10807.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24895.10 ms /   931 tokens (   26.74 ms per token,    37.40 tokens per second)\n",
            "llama_print_timings:        eval time =   16558.51 ms /   142 runs   (  116.61 ms per token,     8.58 tokens per second)\n",
            "llama_print_timings:       total time =   41700.27 ms /  1073 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      20.13 ms /   221 runs   (    0.09 ms per token, 10980.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17783.53 ms /   709 tokens (   25.08 ms per token,    39.87 tokens per second)\n",
            "llama_print_timings:        eval time =   24592.97 ms /   220 runs   (  111.79 ms per token,     8.95 tokens per second)\n",
            "llama_print_timings:       total time =   42755.42 ms /   929 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      36.38 ms /   410 runs   (    0.09 ms per token, 11271.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20429.17 ms /   883 tokens (   23.14 ms per token,    43.22 tokens per second)\n",
            "llama_print_timings:        eval time =   46883.04 ms /   409 runs   (  114.63 ms per token,     8.72 tokens per second)\n",
            "llama_print_timings:       total time =   68045.23 ms /  1292 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      38.35 ms /   428 runs   (    0.09 ms per token, 11160.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21266.60 ms /   883 tokens (   24.08 ms per token,    41.52 tokens per second)\n",
            "llama_print_timings:        eval time =   49688.65 ms /   427 runs   (  116.37 ms per token,     8.59 tokens per second)\n",
            "llama_print_timings:       total time =   71706.57 ms /  1310 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      29.92 ms /   325 runs   (    0.09 ms per token, 10862.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7518.33 ms /   292 tokens (   25.75 ms per token,    38.84 tokens per second)\n",
            "llama_print_timings:        eval time =   36975.41 ms /   324 runs   (  114.12 ms per token,     8.76 tokens per second)\n",
            "llama_print_timings:       total time =   45075.66 ms /   616 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      21.51 ms /   233 runs   (    0.09 ms per token, 10831.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19214.21 ms /   767 tokens (   25.05 ms per token,    39.92 tokens per second)\n",
            "llama_print_timings:        eval time =   27340.05 ms /   232 runs   (  117.85 ms per token,     8.49 tokens per second)\n",
            "llama_print_timings:       total time =   46966.19 ms /   999 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      17.50 ms /   186 runs   (    0.09 ms per token, 10630.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12154.15 ms /   477 tokens (   25.48 ms per token,    39.25 tokens per second)\n",
            "llama_print_timings:        eval time =   20618.60 ms /   185 runs   (  111.45 ms per token,     8.97 tokens per second)\n",
            "llama_print_timings:       total time =   33088.19 ms /   662 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      41.82 ms /   479 runs   (    0.09 ms per token, 11453.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24113.60 ms /   974 tokens (   24.76 ms per token,    40.39 tokens per second)\n",
            "llama_print_timings:        eval time =   58068.68 ms /   478 runs   (  121.48 ms per token,     8.23 tokens per second)\n",
            "llama_print_timings:       total time =   83097.96 ms /  1452 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      46.74 ms /   512 runs   (    0.09 ms per token, 10953.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21269.10 ms /   836 tokens (   25.44 ms per token,    39.31 tokens per second)\n",
            "llama_print_timings:        eval time =   61154.44 ms /   511 runs   (  119.68 ms per token,     8.36 tokens per second)\n",
            "llama_print_timings:       total time =   83394.46 ms /  1347 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      19.24 ms /   212 runs   (    0.09 ms per token, 11016.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19573.28 ms /   773 tokens (   25.32 ms per token,    39.49 tokens per second)\n",
            "llama_print_timings:        eval time =   25523.92 ms /   211 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
            "llama_print_timings:       total time =   45492.19 ms /   984 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      33.80 ms /   361 runs   (    0.09 ms per token, 10679.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24251.47 ms /   727 tokens (   33.36 ms per token,    29.98 tokens per second)\n",
            "llama_print_timings:        eval time =   43020.33 ms /   360 runs   (  119.50 ms per token,     8.37 tokens per second)\n",
            "llama_print_timings:       total time =   67953.44 ms /  1087 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      41.76 ms /   452 runs   (    0.09 ms per token, 10822.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19693.87 ms /   705 tokens (   27.93 ms per token,    35.80 tokens per second)\n",
            "llama_print_timings:        eval time =   54221.38 ms /   451 runs   (  120.22 ms per token,     8.32 tokens per second)\n",
            "llama_print_timings:       total time =   74843.67 ms /  1156 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =       8.85 ms /    93 runs   (    0.10 ms per token, 10512.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17007.79 ms /   568 tokens (   29.94 ms per token,    33.40 tokens per second)\n",
            "llama_print_timings:        eval time =   10343.17 ms /    92 runs   (  112.43 ms per token,     8.89 tokens per second)\n",
            "llama_print_timings:       total time =   27524.88 ms /   660 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.41 ms /   303 runs   (    0.09 ms per token, 11052.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23984.11 ms /   911 tokens (   26.33 ms per token,    37.98 tokens per second)\n",
            "llama_print_timings:        eval time =   37129.83 ms /   302 runs   (  122.95 ms per token,     8.13 tokens per second)\n",
            "llama_print_timings:       total time =   61699.08 ms /  1213 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      30.02 ms /   327 runs   (    0.09 ms per token, 10893.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =   22984.08 ms /   891 tokens (   25.80 ms per token,    38.77 tokens per second)\n",
            "llama_print_timings:        eval time =   39692.41 ms /   326 runs   (  121.76 ms per token,     8.21 tokens per second)\n",
            "llama_print_timings:       total time =   63322.43 ms /  1217 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      29.57 ms /   325 runs   (    0.09 ms per token, 10989.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9647.88 ms /   292 tokens (   33.04 ms per token,    30.27 tokens per second)\n",
            "llama_print_timings:        eval time =   36860.93 ms /   324 runs   (  113.77 ms per token,     8.79 tokens per second)\n",
            "llama_print_timings:       total time =   47111.57 ms /   616 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      38.88 ms /   429 runs   (    0.09 ms per token, 11032.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21788.68 ms /   891 tokens (   24.45 ms per token,    40.89 tokens per second)\n",
            "llama_print_timings:        eval time =   49291.27 ms /   428 runs   (  115.17 ms per token,     8.68 tokens per second)\n",
            "llama_print_timings:       total time =   71901.86 ms /  1319 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      22.10 ms /   253 runs   (    0.09 ms per token, 11449.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =   51581.00 ms /  1963 tokens (   26.28 ms per token,    38.06 tokens per second)\n",
            "llama_print_timings:        eval time =   31898.55 ms /   252 runs   (  126.58 ms per token,     7.90 tokens per second)\n",
            "llama_print_timings:       total time =   83951.06 ms /  2215 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      39.47 ms /   436 runs   (    0.09 ms per token, 11045.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10626.18 ms /   410 tokens (   25.92 ms per token,    38.58 tokens per second)\n",
            "llama_print_timings:        eval time =   50841.03 ms /   435 runs   (  116.88 ms per token,     8.56 tokens per second)\n",
            "llama_print_timings:       total time =   62288.77 ms /   845 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      34.34 ms /   366 runs   (    0.09 ms per token, 10659.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17031.50 ms /   628 tokens (   27.12 ms per token,    36.87 tokens per second)\n",
            "llama_print_timings:        eval time =   42514.50 ms /   365 runs   (  116.48 ms per token,     8.59 tokens per second)\n",
            "llama_print_timings:       total time =   60227.78 ms /   993 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      11.45 ms /   124 runs   (    0.09 ms per token, 10826.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12416.26 ms /   438 tokens (   28.35 ms per token,    35.28 tokens per second)\n",
            "llama_print_timings:        eval time =   13409.96 ms /   123 runs   (  109.02 ms per token,     9.17 tokens per second)\n",
            "llama_print_timings:       total time =   26051.53 ms /   561 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      20.54 ms /   230 runs   (    0.09 ms per token, 11196.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21489.78 ms /   882 tokens (   24.36 ms per token,    41.04 tokens per second)\n",
            "llama_print_timings:        eval time =   27239.86 ms /   229 runs   (  118.95 ms per token,     8.41 tokens per second)\n",
            "llama_print_timings:       total time =   49141.84 ms /  1111 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      37.95 ms /   422 runs   (    0.09 ms per token, 11120.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18249.15 ms /   719 tokens (   25.38 ms per token,    39.40 tokens per second)\n",
            "llama_print_timings:        eval time =   49776.21 ms /   421 runs   (  118.23 ms per token,     8.46 tokens per second)\n",
            "llama_print_timings:       total time =   68840.86 ms /  1140 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      28.24 ms /   307 runs   (    0.09 ms per token, 10869.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23739.09 ms /   903 tokens (   26.29 ms per token,    38.04 tokens per second)\n",
            "llama_print_timings:        eval time =   36520.49 ms /   306 runs   (  119.35 ms per token,     8.38 tokens per second)\n",
            "llama_print_timings:       total time =   60845.47 ms /  1209 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      32.15 ms /   360 runs   (    0.09 ms per token, 11196.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =   57538.62 ms /  1945 tokens (   29.58 ms per token,    33.80 tokens per second)\n",
            "llama_print_timings:        eval time =   48716.88 ms /   359 runs   (  135.70 ms per token,     7.37 tokens per second)\n",
            "llama_print_timings:       total time =  106938.10 ms /  2304 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      26.98 ms /   299 runs   (    0.09 ms per token, 11080.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =   36753.66 ms /  1387 tokens (   26.50 ms per token,    37.74 tokens per second)\n",
            "llama_print_timings:        eval time =   38571.16 ms /   298 runs   (  129.43 ms per token,     7.73 tokens per second)\n",
            "llama_print_timings:       total time =   75899.53 ms /  1685 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      28.81 ms /   317 runs   (    0.09 ms per token, 11004.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   22017.13 ms /   843 tokens (   26.12 ms per token,    38.29 tokens per second)\n",
            "llama_print_timings:        eval time =   38137.66 ms /   316 runs   (  120.69 ms per token,     8.29 tokens per second)\n",
            "llama_print_timings:       total time =   60764.55 ms /  1159 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      33.44 ms /   357 runs   (    0.09 ms per token, 10675.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17377.88 ms /   595 tokens (   29.21 ms per token,    34.24 tokens per second)\n",
            "llama_print_timings:        eval time =   41442.46 ms /   356 runs   (  116.41 ms per token,     8.59 tokens per second)\n",
            "llama_print_timings:       total time =   59531.88 ms /   951 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.94 ms /   311 runs   (    0.09 ms per token, 11130.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   32470.98 ms /  1184 tokens (   27.42 ms per token,    36.46 tokens per second)\n",
            "llama_print_timings:        eval time =   37020.84 ms /   310 runs   (  119.42 ms per token,     8.37 tokens per second)\n",
            "llama_print_timings:       total time =   70058.10 ms /  1494 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      34.36 ms /   377 runs   (    0.09 ms per token, 10972.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =   28199.74 ms /  1074 tokens (   26.26 ms per token,    38.09 tokens per second)\n",
            "llama_print_timings:        eval time =   43974.63 ms /   376 runs   (  116.95 ms per token,     8.55 tokens per second)\n",
            "llama_print_timings:       total time =   72889.34 ms /  1450 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      31.13 ms /   358 runs   (    0.09 ms per token, 11500.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =   37419.44 ms /  1455 tokens (   25.72 ms per token,    38.88 tokens per second)\n",
            "llama_print_timings:        eval time =   44331.07 ms /   357 runs   (  124.18 ms per token,     8.05 tokens per second)\n",
            "llama_print_timings:       total time =   82414.31 ms /  1812 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      28.33 ms /   344 runs   (    0.08 ms per token, 12143.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =   38457.43 ms /  1529 tokens (   25.15 ms per token,    39.76 tokens per second)\n",
            "llama_print_timings:        eval time =   42667.86 ms /   343 runs   (  124.40 ms per token,     8.04 tokens per second)\n",
            "llama_print_timings:       total time =   81738.82 ms /  1872 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      28.87 ms /   322 runs   (    0.09 ms per token, 11151.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18591.59 ms /   694 tokens (   26.79 ms per token,    37.33 tokens per second)\n",
            "llama_print_timings:        eval time =   36078.67 ms /   321 runs   (  112.39 ms per token,     8.90 tokens per second)\n",
            "llama_print_timings:       total time =   55259.61 ms /  1015 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      23.61 ms /   282 runs   (    0.08 ms per token, 11945.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   47284.60 ms /  1839 tokens (   25.71 ms per token,    38.89 tokens per second)\n",
            "llama_print_timings:        eval time =   34915.13 ms /   281 runs   (  124.25 ms per token,     8.05 tokens per second)\n",
            "llama_print_timings:       total time =   82705.56 ms /  2120 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      19.59 ms /   234 runs   (    0.08 ms per token, 11946.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16367.06 ms /   608 tokens (   26.92 ms per token,    37.15 tokens per second)\n",
            "llama_print_timings:        eval time =   24861.93 ms /   233 runs   (  106.70 ms per token,     9.37 tokens per second)\n",
            "llama_print_timings:       total time =   41635.08 ms /   841 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      35.32 ms /   419 runs   (    0.08 ms per token, 11864.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18400.54 ms /   718 tokens (   25.63 ms per token,    39.02 tokens per second)\n",
            "llama_print_timings:        eval time =   46197.25 ms /   418 runs   (  110.52 ms per token,     9.05 tokens per second)\n",
            "llama_print_timings:       total time =   65358.05 ms /  1136 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      28.79 ms /   342 runs   (    0.08 ms per token, 11880.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19944.66 ms /   818 tokens (   24.38 ms per token,    41.01 tokens per second)\n",
            "llama_print_timings:        eval time =   38292.98 ms /   341 runs   (  112.30 ms per token,     8.91 tokens per second)\n",
            "llama_print_timings:       total time =   58828.47 ms /  1159 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      19.72 ms /   227 runs   (    0.09 ms per token, 11512.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16295.34 ms /   606 tokens (   26.89 ms per token,    37.19 tokens per second)\n",
            "llama_print_timings:        eval time =   23886.96 ms /   226 runs   (  105.69 ms per token,     9.46 tokens per second)\n",
            "llama_print_timings:       total time =   40586.77 ms /   832 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      26.21 ms /   311 runs   (    0.08 ms per token, 11863.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =   30797.02 ms /  1189 tokens (   25.90 ms per token,    38.61 tokens per second)\n",
            "llama_print_timings:        eval time =   36159.93 ms /   310 runs   (  116.64 ms per token,     8.57 tokens per second)\n",
            "llama_print_timings:       total time =   67519.49 ms /  1499 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      36.89 ms /   426 runs   (    0.09 ms per token, 11548.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14278.76 ms /   532 tokens (   26.84 ms per token,    37.26 tokens per second)\n",
            "llama_print_timings:        eval time =   45710.38 ms /   425 runs   (  107.55 ms per token,     9.30 tokens per second)\n",
            "llama_print_timings:       total time =   60767.05 ms /   957 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      29.04 ms /   341 runs   (    0.09 ms per token, 11742.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =   30288.78 ms /  1176 tokens (   25.76 ms per token,    38.83 tokens per second)\n",
            "llama_print_timings:        eval time =   39521.69 ms /   340 runs   (  116.24 ms per token,     8.60 tokens per second)\n",
            "llama_print_timings:       total time =   70412.43 ms /  1516 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      37.70 ms /   434 runs   (    0.09 ms per token, 11510.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21482.03 ms /   881 tokens (   24.38 ms per token,    41.01 tokens per second)\n",
            "llama_print_timings:        eval time =   49332.77 ms /   433 runs   (  113.93 ms per token,     8.78 tokens per second)\n",
            "llama_print_timings:       total time =   71588.09 ms /  1314 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      37.94 ms /   432 runs   (    0.09 ms per token, 11387.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24700.83 ms /   977 tokens (   25.28 ms per token,    39.55 tokens per second)\n",
            "llama_print_timings:        eval time =   49945.78 ms /   431 runs   (  115.88 ms per token,     8.63 tokens per second)\n",
            "llama_print_timings:       total time =   75449.60 ms /  1408 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.82 ms /   319 runs   (    0.09 ms per token, 11465.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =   32694.41 ms /  1293 tokens (   25.29 ms per token,    39.55 tokens per second)\n",
            "llama_print_timings:        eval time =   38197.41 ms /   318 runs   (  120.12 ms per token,     8.33 tokens per second)\n",
            "llama_print_timings:       total time =   71473.43 ms /  1611 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      30.36 ms /   359 runs   (    0.08 ms per token, 11824.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =   29478.57 ms /  1136 tokens (   25.95 ms per token,    38.54 tokens per second)\n",
            "llama_print_timings:        eval time =   41943.62 ms /   358 runs   (  117.16 ms per token,     8.54 tokens per second)\n",
            "llama_print_timings:       total time =   72065.38 ms /  1494 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "61/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      31.89 ms /   341 runs   (    0.09 ms per token, 10692.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23277.68 ms /   863 tokens (   26.97 ms per token,    37.07 tokens per second)\n",
            "llama_print_timings:        eval time =   64349.94 ms /   340 runs   (  189.26 ms per token,     5.28 tokens per second)\n",
            "llama_print_timings:       total time =   88284.78 ms /  1203 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      15.38 ms /   178 runs   (    0.09 ms per token, 11576.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20445.21 ms /   764 tokens (   26.76 ms per token,    37.37 tokens per second)\n",
            "llama_print_timings:        eval time =   20172.55 ms /   177 runs   (  113.97 ms per token,     8.77 tokens per second)\n",
            "llama_print_timings:       total time =   40942.22 ms /   941 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      31.68 ms /   356 runs   (    0.09 ms per token, 11237.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20946.25 ms /   819 tokens (   25.58 ms per token,    39.10 tokens per second)\n",
            "llama_print_timings:        eval time =   42222.86 ms /   355 runs   (  118.94 ms per token,     8.41 tokens per second)\n",
            "llama_print_timings:       total time =   63793.21 ms /  1174 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      33.57 ms /   390 runs   (    0.09 ms per token, 11618.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21172.83 ms /   801 tokens (   26.43 ms per token,    37.83 tokens per second)\n",
            "llama_print_timings:        eval time =   46093.09 ms /   389 runs   (  118.49 ms per token,     8.44 tokens per second)\n",
            "llama_print_timings:       total time =   67995.99 ms /  1190 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      20.41 ms /   227 runs   (    0.09 ms per token, 11123.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17931.84 ms /   639 tokens (   28.06 ms per token,    35.63 tokens per second)\n",
            "llama_print_timings:        eval time =   25589.90 ms /   226 runs   (  113.23 ms per token,     8.83 tokens per second)\n",
            "llama_print_timings:       total time =   43939.45 ms /   865 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "66/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      33.95 ms /   391 runs   (    0.09 ms per token, 11515.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =   30509.53 ms /  1082 tokens (   28.20 ms per token,    35.46 tokens per second)\n",
            "llama_print_timings:        eval time =   53610.90 ms /   390 runs   (  137.46 ms per token,     7.27 tokens per second)\n",
            "llama_print_timings:       total time =   84853.90 ms /  1472 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      26.64 ms /   287 runs   (    0.09 ms per token, 10771.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20462.22 ms /   557 tokens (   36.74 ms per token,    27.22 tokens per second)\n",
            "llama_print_timings:        eval time =   35544.36 ms /   286 runs   (  124.28 ms per token,     8.05 tokens per second)\n",
            "llama_print_timings:       total time =   56548.10 ms /   843 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "68/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      31.00 ms /   364 runs   (    0.09 ms per token, 11741.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =   33579.18 ms /  1183 tokens (   28.38 ms per token,    35.23 tokens per second)\n",
            "llama_print_timings:        eval time =   46871.18 ms /   363 runs   (  129.12 ms per token,     7.74 tokens per second)\n",
            "llama_print_timings:       total time =   81112.03 ms /  1546 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "69/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      20.03 ms /   226 runs   (    0.09 ms per token, 11280.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17512.46 ms /   574 tokens (   30.51 ms per token,    32.78 tokens per second)\n",
            "llama_print_timings:        eval time =   25966.27 ms /   225 runs   (  115.41 ms per token,     8.67 tokens per second)\n",
            "llama_print_timings:       total time =   43905.62 ms /   799 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      22.64 ms /   256 runs   (    0.09 ms per token, 11306.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26691.61 ms /  1018 tokens (   26.22 ms per token,    38.14 tokens per second)\n",
            "llama_print_timings:        eval time =   31204.39 ms /   255 runs   (  122.37 ms per token,     8.17 tokens per second)\n",
            "llama_print_timings:       total time =   58388.11 ms /  1273 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      29.20 ms /   325 runs   (    0.09 ms per token, 11130.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20310.50 ms /   747 tokens (   27.19 ms per token,    36.78 tokens per second)\n",
            "llama_print_timings:        eval time =   40703.04 ms /   324 runs   (  125.63 ms per token,     7.96 tokens per second)\n",
            "llama_print_timings:       total time =   61624.83 ms /  1071 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      24.61 ms /   278 runs   (    0.09 ms per token, 11298.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25336.95 ms /   958 tokens (   26.45 ms per token,    37.81 tokens per second)\n",
            "llama_print_timings:        eval time =   32264.65 ms /   277 runs   (  116.48 ms per token,     8.59 tokens per second)\n",
            "llama_print_timings:       total time =   58102.82 ms /  1235 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      23.69 ms /   271 runs   (    0.09 ms per token, 11439.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23619.88 ms /   934 tokens (   25.29 ms per token,    39.54 tokens per second)\n",
            "llama_print_timings:        eval time =   30777.69 ms /   270 runs   (  113.99 ms per token,     8.77 tokens per second)\n",
            "llama_print_timings:       total time =   54864.02 ms /  1204 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      32.90 ms /   363 runs   (    0.09 ms per token, 11035.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26365.65 ms /  1034 tokens (   25.50 ms per token,    39.22 tokens per second)\n",
            "llama_print_timings:        eval time =   41922.12 ms /   362 runs   (  115.81 ms per token,     8.64 tokens per second)\n",
            "llama_print_timings:       total time =   68921.25 ms /  1396 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      34.67 ms /   401 runs   (    0.09 ms per token, 11565.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =   30291.12 ms /  1174 tokens (   25.80 ms per token,    38.76 tokens per second)\n",
            "llama_print_timings:        eval time =   47312.38 ms /   400 runs   (  118.28 ms per token,     8.45 tokens per second)\n",
            "llama_print_timings:       total time =   78303.59 ms /  1574 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      44.42 ms /   502 runs   (    0.09 ms per token, 11301.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =   29822.46 ms /  1106 tokens (   26.96 ms per token,    37.09 tokens per second)\n",
            "llama_print_timings:        eval time =   58988.15 ms /   501 runs   (  117.74 ms per token,     8.49 tokens per second)\n",
            "llama_print_timings:       total time =   89715.24 ms /  1607 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      30.77 ms /   337 runs   (    0.09 ms per token, 10952.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17854.94 ms /   652 tokens (   27.38 ms per token,    36.52 tokens per second)\n",
            "llama_print_timings:        eval time =   37173.54 ms /   336 runs   (  110.64 ms per token,     9.04 tokens per second)\n",
            "llama_print_timings:       total time =   55622.77 ms /   988 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "78/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      11.76 ms /   129 runs   (    0.09 ms per token, 10967.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17995.66 ms /   641 tokens (   28.07 ms per token,    35.62 tokens per second)\n",
            "llama_print_timings:        eval time =   20051.98 ms /   128 runs   (  156.66 ms per token,     6.38 tokens per second)\n",
            "llama_print_timings:       total time =   38269.57 ms /   769 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "79/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      30.38 ms /   321 runs   (    0.09 ms per token, 10567.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   41283.01 ms /   904 tokens (   45.67 ms per token,    21.90 tokens per second)\n",
            "llama_print_timings:        eval time =   68898.35 ms /   320 runs   (  215.31 ms per token,     4.64 tokens per second)\n",
            "llama_print_timings:       total time =  110801.92 ms /  1224 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      17.54 ms /   186 runs   (    0.09 ms per token, 10604.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =   34823.98 ms /  1038 tokens (   33.55 ms per token,    29.81 tokens per second)\n",
            "llama_print_timings:        eval time =   25597.27 ms /   185 runs   (  138.36 ms per token,     7.23 tokens per second)\n",
            "llama_print_timings:       total time =   60763.31 ms /  1223 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "81/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      63.09 ms /   475 runs   (    0.13 ms per token,  7529.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =   29851.25 ms /   964 tokens (   30.97 ms per token,    32.29 tokens per second)\n",
            "llama_print_timings:        eval time =  150495.64 ms /   474 runs   (  317.50 ms per token,     3.15 tokens per second)\n",
            "llama_print_timings:       total time =  181487.49 ms /  1438 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      19.38 ms /   190 runs   (    0.10 ms per token,  9806.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18551.73 ms /   524 tokens (   35.40 ms per token,    28.25 tokens per second)\n",
            "llama_print_timings:        eval time =   39115.75 ms /   189 runs   (  206.96 ms per token,     4.83 tokens per second)\n",
            "llama_print_timings:       total time =   58075.96 ms /   713 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      22.55 ms /   234 runs   (    0.10 ms per token, 10379.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =   35760.23 ms /  1160 tokens (   30.83 ms per token,    32.44 tokens per second)\n",
            "llama_print_timings:        eval time =   39308.37 ms /   233 runs   (  168.71 ms per token,     5.93 tokens per second)\n",
            "llama_print_timings:       total time =   75527.43 ms /  1393 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      13.53 ms /   144 runs   (    0.09 ms per token, 10641.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21466.50 ms /   576 tokens (   37.27 ms per token,    26.83 tokens per second)\n",
            "llama_print_timings:        eval time =   24373.40 ms /   143 runs   (  170.44 ms per token,     5.87 tokens per second)\n",
            "llama_print_timings:       total time =   46106.96 ms /   719 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "85/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      26.99 ms /   263 runs   (    0.10 ms per token,  9743.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =   30475.40 ms /   939 tokens (   32.46 ms per token,    30.81 tokens per second)\n",
            "llama_print_timings:        eval time =   58640.89 ms /   262 runs   (  223.82 ms per token,     4.47 tokens per second)\n",
            "llama_print_timings:       total time =   89642.30 ms /  1201 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      16.05 ms /   162 runs   (    0.10 ms per token, 10094.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25214.51 ms /   538 tokens (   46.87 ms per token,    21.34 tokens per second)\n",
            "llama_print_timings:        eval time =   24276.00 ms /   161 runs   (  150.78 ms per token,     6.63 tokens per second)\n",
            "llama_print_timings:       total time =   49811.06 ms /   699 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.30 ms /   289 runs   (    0.09 ms per token, 10585.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =   45980.83 ms /  1052 tokens (   43.71 ms per token,    22.88 tokens per second)\n",
            "llama_print_timings:        eval time =   72511.73 ms /   288 runs   (  251.78 ms per token,     3.97 tokens per second)\n",
            "llama_print_timings:       total time =  119065.98 ms /  1340 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "88/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      31.98 ms /   335 runs   (    0.10 ms per token, 10475.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =   31947.51 ms /   843 tokens (   37.90 ms per token,    26.39 tokens per second)\n",
            "llama_print_timings:        eval time =   59500.61 ms /   334 runs   (  178.15 ms per token,     5.61 tokens per second)\n",
            "llama_print_timings:       total time =   92089.75 ms /  1177 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "89/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      23.70 ms /   217 runs   (    0.11 ms per token,  9156.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =   31250.14 ms /   861 tokens (   36.30 ms per token,    27.55 tokens per second)\n",
            "llama_print_timings:        eval time =   51330.14 ms /   216 runs   (  237.64 ms per token,     4.21 tokens per second)\n",
            "llama_print_timings:       total time =   83063.32 ms /  1077 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.91 ms /   312 runs   (    0.09 ms per token, 11179.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =   70443.13 ms /  1592 tokens (   44.25 ms per token,    22.60 tokens per second)\n",
            "llama_print_timings:        eval time =   63373.04 ms /   311 runs   (  203.77 ms per token,     4.91 tokens per second)\n",
            "llama_print_timings:       total time =  134600.50 ms /  1903 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      57.67 ms /   512 runs   (    0.11 ms per token,  8877.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =   29164.54 ms /   751 tokens (   38.83 ms per token,    25.75 tokens per second)\n",
            "llama_print_timings:        eval time =  117928.31 ms /   511 runs   (  230.78 ms per token,     4.33 tokens per second)\n",
            "llama_print_timings:       total time =  148234.13 ms /  1262 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "92/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      40.49 ms /   403 runs   (    0.10 ms per token,  9951.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25873.52 ms /   594 tokens (   43.56 ms per token,    22.96 tokens per second)\n",
            "llama_print_timings:        eval time =   57381.07 ms /   402 runs   (  142.74 ms per token,     7.01 tokens per second)\n",
            "llama_print_timings:       total time =   84045.11 ms /   996 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      23.05 ms /   253 runs   (    0.09 ms per token, 10978.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =   32663.24 ms /  1148 tokens (   28.45 ms per token,    35.15 tokens per second)\n",
            "llama_print_timings:        eval time =   43695.97 ms /   252 runs   (  173.40 ms per token,     5.77 tokens per second)\n",
            "llama_print_timings:       total time =   76900.51 ms /  1400 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      43.10 ms /   378 runs   (    0.11 ms per token,  8770.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =   53037.58 ms /  1406 tokens (   37.72 ms per token,    26.51 tokens per second)\n",
            "llama_print_timings:        eval time =  154601.31 ms /   377 runs   (  410.08 ms per token,     2.44 tokens per second)\n",
            "llama_print_timings:       total time =  208691.81 ms /  1783 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      24.42 ms /   286 runs   (    0.09 ms per token, 11712.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =   38706.03 ms /  1237 tokens (   31.29 ms per token,    31.96 tokens per second)\n",
            "llama_print_timings:        eval time =   40253.19 ms /   285 runs   (  141.24 ms per token,     7.08 tokens per second)\n",
            "llama_print_timings:       total time =   79473.01 ms /  1522 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      24.63 ms /   190 runs   (    0.13 ms per token,  7714.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26493.14 ms /  1015 tokens (   26.10 ms per token,    38.31 tokens per second)\n",
            "llama_print_timings:        eval time =   30596.83 ms /   189 runs   (  161.89 ms per token,     6.18 tokens per second)\n",
            "llama_print_timings:       total time =   57446.88 ms /  1204 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      30.28 ms /   342 runs   (    0.09 ms per token, 11293.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18810.84 ms /   613 tokens (   30.69 ms per token,    32.59 tokens per second)\n",
            "llama_print_timings:        eval time =   44180.47 ms /   341 runs   (  129.56 ms per token,     7.72 tokens per second)\n",
            "llama_print_timings:       total time =   63607.71 ms /   954 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "98/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      15.76 ms /   179 runs   (    0.09 ms per token, 11359.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24615.92 ms /   884 tokens (   27.85 ms per token,    35.91 tokens per second)\n",
            "llama_print_timings:        eval time =   23035.27 ms /   178 runs   (  129.41 ms per token,     7.73 tokens per second)\n",
            "llama_print_timings:       total time =   47952.33 ms /  1062 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      14.50 ms /   162 runs   (    0.09 ms per token, 11172.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21431.56 ms /   639 tokens (   33.54 ms per token,    29.82 tokens per second)\n",
            "llama_print_timings:        eval time =   24866.06 ms /   161 runs   (  154.45 ms per token,     6.47 tokens per second)\n",
            "llama_print_timings:       total time =   46660.48 ms /   800 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      38.59 ms /   449 runs   (    0.09 ms per token, 11636.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   22950.72 ms /   782 tokens (   29.35 ms per token,    34.07 tokens per second)\n",
            "llama_print_timings:        eval time =   60484.09 ms /   448 runs   (  135.01 ms per token,     7.41 tokens per second)\n",
            "llama_print_timings:       total time =   84244.86 ms /  1230 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.55 ms /   305 runs   (    0.09 ms per token, 11070.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =   33193.71 ms /  1069 tokens (   31.05 ms per token,    32.20 tokens per second)\n",
            "llama_print_timings:        eval time =   40499.97 ms /   304 runs   (  133.22 ms per token,     7.51 tokens per second)\n",
            "llama_print_timings:       total time =   74226.05 ms /  1373 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "102/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      40.97 ms /   454 runs   (    0.09 ms per token, 11081.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   29907.96 ms /   659 tokens (   45.38 ms per token,    22.03 tokens per second)\n",
            "llama_print_timings:        eval time =   62309.57 ms /   453 runs   (  137.55 ms per token,     7.27 tokens per second)\n",
            "llama_print_timings:       total time =   93132.25 ms /  1112 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "103/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      43.75 ms /   512 runs   (    0.09 ms per token, 11702.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =   50008.42 ms /  1563 tokens (   32.00 ms per token,    31.25 tokens per second)\n",
            "llama_print_timings:        eval time =   72798.06 ms /   511 runs   (  142.46 ms per token,     7.02 tokens per second)\n",
            "llama_print_timings:       total time =  123896.04 ms /  2074 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      14.66 ms /   172 runs   (    0.09 ms per token, 11729.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19719.17 ms /   791 tokens (   24.93 ms per token,    40.11 tokens per second)\n",
            "llama_print_timings:        eval time =   19792.50 ms /   171 runs   (  115.75 ms per token,     8.64 tokens per second)\n",
            "llama_print_timings:       total time =   39800.64 ms /   962 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "105/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      28.24 ms /   313 runs   (    0.09 ms per token, 11083.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =   30942.92 ms /  1248 tokens (   24.79 ms per token,    40.33 tokens per second)\n",
            "llama_print_timings:        eval time =   39037.89 ms /   312 runs   (  125.12 ms per token,     7.99 tokens per second)\n",
            "llama_print_timings:       total time =   70517.18 ms /  1560 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      27.36 ms /   307 runs   (    0.09 ms per token, 11221.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   31070.87 ms /  1194 tokens (   26.02 ms per token,    38.43 tokens per second)\n",
            "llama_print_timings:        eval time =   36949.15 ms /   306 runs   (  120.75 ms per token,     8.28 tokens per second)\n",
            "llama_print_timings:       total time =   68557.39 ms /  1500 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      31.65 ms /   330 runs   (    0.10 ms per token, 10425.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17027.96 ms /   612 tokens (   27.82 ms per token,    35.94 tokens per second)\n",
            "llama_print_timings:        eval time =   37794.68 ms /   329 runs   (  114.88 ms per token,     8.70 tokens per second)\n",
            "llama_print_timings:       total time =   55406.57 ms /   941 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "108/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      23.73 ms /   263 runs   (    0.09 ms per token, 11080.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10512.28 ms /   410 tokens (   25.64 ms per token,    39.00 tokens per second)\n",
            "llama_print_timings:        eval time =   31554.70 ms /   262 runs   (  120.44 ms per token,     8.30 tokens per second)\n",
            "llama_print_timings:       total time =   42618.09 ms /   672 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      28.12 ms /   321 runs   (    0.09 ms per token, 11416.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11996.48 ms /   456 tokens (   26.31 ms per token,    38.01 tokens per second)\n",
            "llama_print_timings:        eval time =   35105.85 ms /   320 runs   (  109.71 ms per token,     9.12 tokens per second)\n",
            "llama_print_timings:       total time =   47668.30 ms /   776 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      44.81 ms /   512 runs   (    0.09 ms per token, 11426.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =   29086.83 ms /  1049 tokens (   27.73 ms per token,    36.06 tokens per second)\n",
            "llama_print_timings:        eval time =   63096.14 ms /   511 runs   (  123.48 ms per token,     8.10 tokens per second)\n",
            "llama_print_timings:       total time =   93116.11 ms /  1560 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111/112 done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   16833.79 ms\n",
            "llama_print_timings:      sample time =      10.17 ms /   105 runs   (    0.10 ms per token, 10324.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12429.16 ms /   455 tokens (   27.32 ms per token,    36.61 tokens per second)\n",
            "llama_print_timings:        eval time =   11836.94 ms /   104 runs   (  113.82 ms per token,     8.79 tokens per second)\n",
            "llama_print_timings:       total time =   24450.93 ms /   559 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "112/112 done\n"
          ]
        }
      ],
      "source": [
        "df_test = pd.read_csv(file_path_test)\n",
        "\n",
        "responses = []\n",
        "\n",
        "for index, row in df_test.iterrows():\n",
        "    query_str = row['questionText']\n",
        "    response = query_engine.query(query_str)\n",
        "    responses.append(str(response))\n",
        "    print (str(index+1) + \"/\" + str(len(df_test)) + \" done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "33ec2b13",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "4e942db2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test['response'] = responses\n",
        "df_test.to_csv(\"counsel-chat-best-answer-test-response.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d72f5f82",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llama_index_v2",
      "language": "python",
      "name": "llama_index_v2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
